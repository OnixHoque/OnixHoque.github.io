{"title":"A Sparse Approach for Translation-based Training of Knowledge Graph Embeddings","markdown":{"yaml":{"title":"A Sparse Approach for Translation-based Training of Knowledge Graph Embeddings","categories":["TransE","KGE","SpMM","SC24"],"description":"SC24 Best poster finalist. This work accelerates knowledge-graph embedding training by replacing traditional scatter/gather operations with sparse–dense matrix multiplication, reducing memory usage and achieving significant CPU, GPU, and multi-GPU speedups.","date":"11/25/2024","image":"sc24.png"},"headingText":"Background","containsRefs":false,"markdown":"\n\n::: {.callout-note appearance=\"minimal\" title=\"Note\" icon=false collapse=false}\nPhD Research Project \n\n[SC24 Best Poster Finalist](https://sc24.supercomputing.org/proceedings/poster/poster_pages/post275.html){.badge .bg-danger .rounded-pill}\n[Poster (PDF)](https://sc24.supercomputing.org/proceedings/poster/poster_files/post275s2-file2.pdf){.badge .bg-primary .rounded-pill}\n[Paper](https://sc24.supercomputing.org/proceedings/poster/poster_files/post275s2-file3.pdf){.badge .bg-primary .rounded-pill}\n:::\n\n![](presentation.jpeg){.lightbox}\n\n\nIn this work, we explored how to make knowledge graph (KG) embedding training process through expressing it with a more efficient sparse linear algebra kernels. While KG embedding models are widely used, their training process can be extremely time-consuming, particularly for large datasets. Through our analysis, we identify gradient computation of embeddings and vector normalization as the most time-dominating components of the KG embedding training loop. These bottlenecks motivate our investigation into more efficient computation strategies.\n\n## Methodology\n\nTo address the high computational cost, we replace the core embedding operations with SpMM (Sparse–Dense Matrix Multiplication) kernels. This approach unifies multiple scatter and gather operations into a single sparse operation, reducing both training time and memory usage. We apply this sparse computation strategy to the TransE model and evaluate its performance on both CPUs and GPUs. Additionally, we scale the method across a distributed environment using 64 GPUs to assess its effectiveness under large-scale parallelism.\n\n## Findings\n\nOur sparse SpMM-based approach significantly accelerates KG embedding training. On the TransE model, we achieve up to a 5.3× speedup on CPU and up to a 4.2× speedup on GPU. When distributed across 64 GPUs, the method delivers up to a 3.9× improvement per epoch. These results demonstrate that unifying scatter/gather operations through sparse kernels can effectively mitigate training bottlenecks, offering substantial gains in efficiency for large-scale KG learning.\n","srcMarkdownNoYaml":"\n\n::: {.callout-note appearance=\"minimal\" title=\"Note\" icon=false collapse=false}\nPhD Research Project \n\n[SC24 Best Poster Finalist](https://sc24.supercomputing.org/proceedings/poster/poster_pages/post275.html){.badge .bg-danger .rounded-pill}\n[Poster (PDF)](https://sc24.supercomputing.org/proceedings/poster/poster_files/post275s2-file2.pdf){.badge .bg-primary .rounded-pill}\n[Paper](https://sc24.supercomputing.org/proceedings/poster/poster_files/post275s2-file3.pdf){.badge .bg-primary .rounded-pill}\n:::\n\n![](presentation.jpeg){.lightbox}\n\n## Background\n\nIn this work, we explored how to make knowledge graph (KG) embedding training process through expressing it with a more efficient sparse linear algebra kernels. While KG embedding models are widely used, their training process can be extremely time-consuming, particularly for large datasets. Through our analysis, we identify gradient computation of embeddings and vector normalization as the most time-dominating components of the KG embedding training loop. These bottlenecks motivate our investigation into more efficient computation strategies.\n\n## Methodology\n\nTo address the high computational cost, we replace the core embedding operations with SpMM (Sparse–Dense Matrix Multiplication) kernels. This approach unifies multiple scatter and gather operations into a single sparse operation, reducing both training time and memory usage. We apply this sparse computation strategy to the TransE model and evaluate its performance on both CPUs and GPUs. Additionally, we scale the method across a distributed environment using 64 GPUs to assess its effectiveness under large-scale parallelism.\n\n## Findings\n\nOur sparse SpMM-based approach significantly accelerates KG embedding training. On the TransE model, we achieve up to a 5.3× speedup on CPU and up to a 4.2× speedup on GPU. When distributed across 64 GPUs, the method delivers up to a 3.9× improvement per epoch. These results demonstrate that unifying scatter/gather operations through sparse kernels can effectively mitigate training bottlenecks, offering substantial gains in efficiency for large-scale KG learning.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":["cosmo","brand"],"title":"A Sparse Approach for Translation-based Training of Knowledge Graph Embeddings","categories":["TransE","KGE","SpMM","SC24"],"description":"SC24 Best poster finalist. This work accelerates knowledge-graph embedding training by replacing traditional scatter/gather operations with sparse–dense matrix multiplication, reducing memory usage and achieving significant CPU, GPU, and multi-GPU speedups.","date":"11/25/2024","image":"sc24.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}