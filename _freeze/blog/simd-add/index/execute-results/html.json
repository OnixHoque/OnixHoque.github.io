{
  "hash": "648cc69b5e8f5c4118dff8148f52d25e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SIMD Intrinsics in Practice: Measuring Scalar vs SSE2 vs AVX2 Performance\"\ndescription: \"A simple 8-element vector add benchmark shows that AVX2 with proper alignment is fastest, followed by SSE2, unaligned AVX2, and finally scalar code.\"\ncategories: [\"Hands-on\"] \nauthor: \"Md Saidul Hoque Anik\"\ndate: \"01/19/2026\"\nimage: images/paste-1.png\n---\n\nI ran a small benchmark to demonstrate the use of various SIMD intrinsics and to compare their performance. The test measured the speedup of a simple 8-element vector addition implemented in four ways: scalar code, SSE2 intrinsics, AVX2 intrinsics, and AVX2 with explicit alignas memory alignment.\n\nThe results showed a clear performance hierarchy. The AVX2 implementation with aligned data performed best, followed by SSE2, then unaligned AVX2, with the scalar version being the slowest. This highlights how both wider vector units and proper data alignment can have a measurable impact on SIMD performance, even for small workloads.\n\n<!-- ![](images/paste-1.png) -->\n\n::: {#36eeea1d .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=662 height=277}\n:::\n:::\n\n\nUnaligned AVX2 can be slower than SSE2 because the overhead of wider 256-bit operations outweighs their benefits for very small workloads. Unaligned AVX2 loads may require extra micro-operations or split memory accesses, and on many CPUs they can also trigger a lower core frequency when AVX instructions are used. For an 8-element vector add, these costs dominate, while SSE2 avoids AVX downclocking and moves less data per instruction, making it more efficient in this case. Once data is properly aligned or the workload grows, AVX2's wider vectors regain their advantage.\n\n## Scalar\n\n``` c\n#include <stdio.h>\n\nint main() {\n    float a[8] = {1, 2, 3, 4, 5, 6, 7, 8};\n    float b[8] = {10, 20, 30, 40, 50, 60, 70, 80};\n    float result[8];\n\n    // Plain old scalar addition - one element at a time\n    for (int i = 0; i < 8; i++) {\n        result[i] = a[i] + b[i];\n    }\n\n    return 0;\n}\n```\n\n### Run and Profile\n\n``` bash\ngcc -O3 0_scalar_add.c\nperf stat -r 5 -- ./a.out\n```\n\nTime taken: $0.000964 \\pm 0.000319$ seconds\n\n## AVX2 Intrinsics\n\n``` c\n\n#include <immintrin.h>  // AVX2 intrinsics\n#include <stdio.h>\n\nint main() {\n    // Two input arrays of 8 floats each\n    float a[8] = {1, 2, 3, 4, 5, 6, 7, 8};\n    float b[8] = {10, 20, 30, 40, 50, 60, 70, 80};\n    float result[8];\n\n    // Load into 256-bit SIMD registers (ymm)\n    __m256 va = _mm256_loadu_ps(a);\n    __m256 vb = _mm256_loadu_ps(b);\n\n    // Add them in one SIMD instruction\n    __m256 vresult = _mm256_add_ps(va, vb);\n\n    // Store result back to memory\n    _mm256_storeu_ps(result, vresult);\n\n    return 0;\n}\n```\n\n### Run and Profile\n\n``` bash\ngcc -O3 -mavx2 1_vector_add.c\nperf stat -r 5 -- ./a.out\n```\n\nTime taken: $0.0005174 \\pm 0.0000617$ seconds\n\n## AVX2 Intrinsics + Align\n\n``` c\n#include <immintrin.h>\n#include <stdio.h>\n#include <stdalign.h>\n\nint main() {\n    alignas(32) float a[8] = {1,2,3,4,5,6,7,9};\n    alignas(32) float b[8] = {10,20,30,40,50,60,70,90};\n    alignas(32) float result[8];\n\n    _mm256_store_ps(result, _mm256_add_ps(\n        _mm256_load_ps(a),\n        _mm256_load_ps(b)\n    ));\n\n    return 0;\n}\n```\n\n### Run and Profile\n\n``` bash\ngcc -O3 -mavx2 2_vector_add.c\nperf stat -r 5 -- ./a.out\n```\n\nTime taken: $0.0004999 \\pm 0.0000505$ seconds\n\n## SSE2 Intrinsics\n\n``` c\n#include <emmintrin.h>  // SSE2\n#include <stdio.h>\n\nint main() {\n    float a[8] = {1, 2, 3, 4, 5, 6, 7, 8};\n    float b[8] = {10,20,30,40,50,60,70,80};\n    float result[8];\n\n    // First half (elements 0–3)\n    _mm_storeu_ps(result + 0,\n        _mm_add_ps(\n            _mm_loadu_ps(a + 0),\n            _mm_loadu_ps(b + 0)\n        ));\n\n    // Second half (elements 4–7)\n    _mm_storeu_ps(result + 4,\n        _mm_add_ps(\n            _mm_loadu_ps(a + 4),\n            _mm_loadu_ps(b + 4)\n        ));\n\n    return 0;\n}\n```\n\n### Run and Profile\n\n``` bash\ngcc -O3 -msse2 3_vector_add.c\nperf stat -r 5 -- ./a.out\n```\n\nTime taken: $0.0005111 \\pm 0.0000500$ seconds\n\n## Summary\n\nThe following table summarizes the execution time of various implementations—AVX2, SSE2, aligned AVX2, and scalar—and reports their corresponding speedups relative to the scalar baseline. All experiments were conducted on an Intel® Core™ Ultra 9 Processor 285H with 24 MB cache and a maximum turbo frequency of 5.40 GHz.\n\n\n|       Method       |    Time (µs)     |    Speedup    |\n|:------------------:|:----------------:|:-------------:|\n|       Scalar       | $964.0 \\pm 319$  |   $1\\times$   |\n|        AVX2        | $517.4 \\pm 61.7$ | $1.863\\times$ |\n|     AVX2+Align     | $499.9 \\pm 50.5$ | $1.928\\times$ |\n| SSE2+Loop Unrolled | $511.1 \\pm 50.0$ | $1.886\\times$ |\n\n: Benchmarking\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}