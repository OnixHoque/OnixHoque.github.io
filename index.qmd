---
title: "Md Saidul Hoque Anik"
description: "PhD Candidate | Scalable and Differentiable Sparse Kernels for Graph Learning | ML Systems Optimization"
toc: false
format: 
  html:
    page-layout: full
about:
  template: trestles
  image: images/propic2.png
  image-title: "At MLSys 2025"
  # links:
  #   - icon: twitter
  #     text: twitter
  #     href: https://twitter.com
  #   - icon: github
  #     text: Github
  #     href: https://github.com

linestretch: 1.7

---

## About Me

I am a PhD candidate in Computer Science and Engineering at Texas A&M, specializing in high-performance and distributed Machine Learning Systems. Particularly, I map and customize sparse linear algebra libraries to optimize the efficiency of sparse machine-learning pipelines. My industry experience includes an internship at Amazon AWS, where I developed an end-to-end differentiable GPU kernel autotuner for LLM inference, which significantly improved efficiency by applying transfer learning to reduce new kernel tuning time from days to hours. My core PhD research focuses on accelerating Graph Learning models (GNNs, KGEs, GraphRAG) through new distributed methods and customized sparse linear algebra. 

I can:

* Build custom PyTorch training pipelines for Graph Neural Networks using efficient sparse linear algebra.
* Integrate high-performance C++ libraries into PyTorch workflows through LibTorch and PyBind11.
* Develop distributed training pipelines with PyTorch Distributed, including robust disk-based data streaming.
* Profile Python code with native C/C++ backends to identify and resolve performance bottlenecks.

## Research Background

I have in-depth expertise in designing custom PyTorch-based systems. My accepted work at MLSys 2025 details an approach for expressing Knowledge Graph Embedding training via sparse-matrix multiplication, demonstrating innovative system design for large-scale, sparse model efficiency. Furthermore, I developed a high-performance and auto-tuned CPU Sparse Matrix-Matrix Multiplication (SpMM) library that accelerates PyTorch GCN training by up to 93x (published at ACM WebConf 2024) and supports several GNNs along with multiple CPU architectures (Intel, AMD, ARM). I am currently collaborating with Oak Ridge National Lab to build a distributed and differentiable framework for large-scale KGE training across hybrid storage and compute tiers (disk, CPU, GPU).


<!-- :::{.column-margin} -->
## Featured Research

* [SC24 best poster finalist (top-6): Expressed TransE Knowledge Graph Embedding (KGE) training using faster sparse-dense matrix multiplication kernel.](/research/systems/efficient-kge/index.qmd)
* [MLSys 2025 Paper: Generalized 10 KGE model training using faster sparse-dense matrix multiplication kernel.](/research/systems/sptransx/index.qmd)
* [Amazon Summer Internship 2025: Developed Differentiable GPU Kernel Autotuner.](/research/systems/gpu-tuner-amazon/index.qmd)

See all 20+ research projects [here](/research/index.qmd). Or, see an overview of my progress [here](/news/index.qmd).

<!-- ::: -->