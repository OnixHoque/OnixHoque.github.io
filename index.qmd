---
title: "Md Saidul Hoque Anik"
description: "PhD Candidate | Sparse Kernels for Graph Learning | ML Systems & Performance Optimization"
toc: false
format: 
  html:
    page-layout: full
about:
  template: trestles
  image: images/propic2.png
  image-title: "At MLSys 2025"
  links:
    - icon: github
      text: Github
      href: https://github.com/OnixHoque
    - icon: linkedin
      text: LinkedIn 
      href: https://www.linkedin.com/in/onixhoque/

# linestretch: 1.7

---
## About Me

I am a PhD candidate in Computer Science & Engineering at Texas A&M University, focusing on **high-performance and distributed machine learning systems**. I specialize in optimizing **sparse linear algebra** for efficient ML pipelines. Previously, I interned at **Amazon AWS**, where I built a differentiable GPU kernel autotuner that reduced LLM kernel-tuning time from **days to hours** using transfer learning.


> I can:
>
> - Build custom **PyTorch GNN** training pipelines backed by optimized sparse linear algebra.  
> - Integrate custom **C++/CUDA** kernels into PyTorch with LibTorch & PyBind11.  
> - Develop **distributed training systems** with PyTorch Distributed and efficient disk-based data streaming.  
> - Profile and optimize Python/C++ pipelines for performance bottlenecks.  
> - Apply ML for systems optimization (e.g., neural GPU kernel autotuning).  


## Research Background

My research centers on **scalable graph learning systems** (GNNs, KGEs, GraphRAG) and custom sparse linear algebra.  
At **MLSys 2025**, I presented a generalized method for expressing KGE training models through sparse matrix multiplication to improve large-scale efficiency.  
I also built a **high-performance CPU SpMM library** (ACM WebConf 2024) that accelerates PyTorch GNN training by **up to 93Ã—** across Intel, AMD, and ARM CPUs.  
Currently, I am collaborating with **Oak Ridge National Lab** on a distributed, differentiable framework for large-scale KGE training across hybrid compute tiers.

<!-- :::{.column-margin} -->
## Featured Research

* [SC24 best poster finalist (top-6): Expressed TransE Knowledge Graph Embedding (KGE) training using faster sparse-dense matrix multiplication kernel.](/research/systems/efficient-kge/index.qmd)
* [MLSys 2025 Paper: Generalized 10 KGE model training using faster sparse-dense matrix multiplication kernel.](/research/systems/sptransx/index.qmd)
* [Amazon Summer Internship 2025: Developed Differentiable GPU Kernel Autotuner.](/research/systems/gpu-tuner-amazon/index.qmd)

Explore all 20+ research projects [here](/research/index.qmd), or check out a quick progress overview [here](/news/index.qmd).


<!-- ::: -->